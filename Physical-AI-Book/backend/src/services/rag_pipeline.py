"""
RAG Pipeline orchestration service.
Coordinates embedding, retrieval, and LLM generation for question answering.
"""
import time
from typing import List, Dict, Optional, Tuple
from uuid import UUID

from src.services.embeddings import embedding_service
from src.services.qdrant_service import qdrant_service
from src.services.postgres_service import postgres_service
from src.services.agent_service import agent_service
from src.config import settings


class RAGPipeline:
    """Orchestrates RAG workflow: embed -> retrieve -> generate."""

    SIMILARITY_THRESHOLD = 0.35  # Minimum cosine similarity for relevant results

    def __init__(self):
        """Initialize RAG pipeline with service dependencies."""
        self.embeddings = embedding_service
        self.qdrant = qdrant_service
        self.postgres = postgres_service
        self.agent = agent_service
        self.max_chunks = settings.max_retrieval_chunks

    async def process_query(
        self,
        query: str,
        highlighted_text: Optional[str] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        filters: Optional[Dict[str, str]] = None,
        user_context: Optional[str] = None
    ) -> Tuple[str, List[Dict[str, any]], int]:
        """
        Process user query through RAG pipeline using OpenAI Agents SDK.

        Args:
            query: User's question
            highlighted_text: Optional text highlighted by user for context-aware explanation
            conversation_history: Optional previous conversation messages
            filters: Optional filters (e.g., {"module_name": "inverse-kinematics"})
            user_context: Optional user background context for personalization

        Returns:
            Tuple of (response_text, source_references, processing_time_ms)
        """
        start_time = time.time()

        # Prepare query - combine with highlighted text if provided
        import logging
        logger = logging.getLogger(__name__)

        final_query = query
        if highlighted_text:
            logger.info(f"Highlighted text received ({len(highlighted_text)} chars): {highlighted_text[:100]}...")
            final_query = f"Context: {highlighted_text}\n\nQuestion: {query}"
        else:
            logger.info("No highlighted text in request")

        # Step 1: Generate embedding for query
        query_embedding = self.embeddings.generate_embedding(final_query)

        # Step 2: Search Qdrant for similar vectors
        search_results = self.qdrant.search(
            query_vector=query_embedding,
            limit=self.max_chunks,
            score_threshold=self.SIMILARITY_THRESHOLD
        )

        # Step 3: Retrieve metadata from Postgres if we have results
        enriched_chunks = []
        if search_results:
            vector_ids = [UUID(str(result.id)) for result in search_results]
            chunks_metadata = await self.postgres.get_chunks_by_vector_ids(vector_ids)
            enriched_chunks = self._merge_results(search_results, chunks_metadata)

        # Step 4: Use agent to generate response (agent will also call retrieval, but we have sources)
        try:
            response = await self.agent.generate_response(
                query=final_query,
                conversation_history=conversation_history,
                user_context=user_context
            )
        except Exception as e:
            raise Exception(f"Agent execution failed: {str(e)}")

        # Step 5: Extract sources from our retrieval
        source_references = self._extract_sources(enriched_chunks) if enriched_chunks else []

        # Calculate processing time
        processing_time = int((time.time() - start_time) * 1000)

        return response, source_references, processing_time

    async def process_query_stream(
        self,
        query: str,
        highlighted_text: Optional[str] = None,
        conversation_history: Optional[List[Dict[str, str]]] = None
    ):
        """
        Process user query through RAG pipeline with streaming response.

        Args:
            query: User's question
            highlighted_text: Optional text highlighted by user for context-aware explanation
            conversation_history: Optional previous conversation messages

        Yields:
            Text chunks as they are generated by the agent
        """
        # Prepare query - combine with highlighted text if provided
        import logging
        logger = logging.getLogger(__name__)

        final_query = query
        if highlighted_text:
            logger.info(f"Highlighted text received ({len(highlighted_text)} chars): {highlighted_text[:100]}...")
            final_query = f"Context: {highlighted_text}\n\nQuestion: {query}"
        else:
            logger.info("No highlighted text in request")

        # Use agent to generate streaming response
        try:
            async for chunk in self.agent.generate_response_stream(
                query=final_query,
                conversation_history=conversation_history
            ):
                yield chunk
        except Exception as e:
            raise Exception(f"Agent streaming failed: {str(e)}")

    def _merge_results(
        self,
        vector_results: List[Dict],
        metadata_results: List[Dict]
    ) -> List[Dict[str, any]]:
        """
        Merge Qdrant search results with Postgres metadata.

        Args:
            vector_results: Results from Qdrant with id and score
            metadata_results: Metadata from Postgres

        Returns:
            List of enriched chunks with all data
        """
        # Create lookup map for metadata
        metadata_map = {
            str(chunk["qdrant_vector_id"]): chunk
            for chunk in metadata_results
        }

        enriched = []
        for result in vector_results:
            vector_id = str(result.id)
            metadata = metadata_map.get(vector_id)

            if metadata:
                enriched.append({
                    "vector_id": vector_id,
                    "score": result.score,
                    "chunk_text": metadata["chunk_text"],
                    "module_name": metadata["module_name"],
                    "chapter_name": metadata["chapter_name"],
                    "section_heading": metadata.get("section_heading"),
                    "file_path": metadata["file_path"],
                    "slug": metadata["slug"],
                    "chunk_index": metadata["chunk_index"],
                })

        return enriched

    def _apply_filters(
        self,
        chunks: List[Dict[str, any]],
        filters: Dict[str, str]
    ) -> List[Dict[str, any]]:
        """
        Apply metadata filters to chunks.

        Args:
            chunks: List of enriched chunks
            filters: Filter criteria (e.g., {"module_name": "kinematics"})

        Returns:
            Filtered list of chunks
        """
        filtered = chunks

        if "module_name" in filters:
            filtered = [c for c in filtered if c["module_name"] == filters["module_name"]]

        if "chapter_name" in filters:
            filtered = [c for c in filtered if c["chapter_name"] == filters["chapter_name"]]

        return filtered

    def _extract_sources(self, chunks: List[Dict[str, any]]) -> List[Dict[str, str]]:
        """
        Extract unique source references from chunks.

        Args:
            chunks: List of enriched chunks

        Returns:
            List of source reference dictionaries
        """
        # Use dict to deduplicate by slug while preserving order
        sources_map = {}

        for chunk in chunks:
            slug = chunk["slug"]
            if slug not in sources_map:
                sources_map[slug] = {
                    "module_name": chunk["module_name"],
                    "chapter_name": chunk["chapter_name"],
                    "slug": slug,
                    "url": f"/docs/{slug}",  # Docusaurus URL format
                    "preview": chunk["chunk_text"][:150] + "..." if len(chunk["chunk_text"]) > 150 else chunk["chunk_text"]
                }

        return list(sources_map.values())


# Global pipeline instance
rag_pipeline = RAGPipeline()
